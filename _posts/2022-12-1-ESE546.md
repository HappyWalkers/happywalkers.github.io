---
title: Positional Embedding in Transformer

article_header:
  type: cover
  image:
    src: https://s2.loli.net/2022/12/30/zt1AsGa4ke79jhp.png 

tags: Deep Learning
cover: https://s2.loli.net/2022/12/30/zt1AsGa4ke79jhp.png 
coverWidth: 1200
coverHeight: 750
---


We prove that the summation of positional embedding and word embedding is a specific case of concatenation

# Report

<div class="row">
	<iframe src="https://drive.google.com/file/d/1liG3Q6Iu48VPxd0GTaOPpJbXWVtvJIfM/preview" style="width:100%; height:550px"></iframe>
</div>

# Source code

https://github.com/HappyWalkers/ESE546Project.git
